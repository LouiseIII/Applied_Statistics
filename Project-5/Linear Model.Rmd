---
title: "Linear Trend"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE)
library(ggplot2)
library(forecast)
library(gridExtra)
library(tidyverse)
library(MASS)
library(stats)
library(cowplot)
library(dplyr)
library(tidyr)
library(readr)
library(pROC)
library(nlme)
```

In this document we will fit a SARIMA model on the data from 1960 to 2022 with a linear trend, as it is shown in the Figure 1 below. 

```{r, fig.align="center", fig.cap="Figure 1: Illustration of the fitted trend using linear Model."}
# our first aim is to identify and subsequently remove it. To achieve this, we will attempt to fit the data to both a linear and quadratic over time model. The resulting fitting curves : 
setwd("~/Google Drive/EPFL/Applied Stat/StatApp-314577/Project-5")
data <- read.csv("temperature.csv", sep=",", dec=".", head=TRUE)

x <- as.vector(t(data[, -c(1, 14:19)]))
x <- ts(x, start=1880, frequency=12)

x <- ts(x[961:1716], start=1960, frequency=12)
lmData <- data.frame(y = x,
                     month = as.factor(rep(1:12,length(x)/12)),
                      t = 1:length(x))

# Fit with linear 

lmfit <- lm(y~., data=lmData)
#summary(lmfit)
#anova(lmfit)
#plot(x, main="de")
#points(seq(1960, 2022, length=length(x)),fitted(lmfit), type="l", col="blue")

g1 <- ggplot(lmData, aes(x=seq(1960, 2022, length=length(x))))+
  geom_line(aes(y=x))+
  geom_line(aes(y=fitted(lmfit)), color="red")+
  theme_minimal()+
  xlab("Time")+
  ylab("Temperature")

# Fit with quadratic 

lmfit2 <- lm(y~.+I(t^2), data=lmData) #+ I(exp(t-500))

g2 <- ggplot(lmData, aes(x=seq(1960, 2022, length=length(x))))+
  geom_line(aes(y=x))+
  geom_line(aes(y=fitted(lmfit2)), color="blue")+
  theme_minimal()+
  xlab("Time")+
  ylab("Temperature")

grid.arrange(g1, ncol = 1)

```

```{r, eval =FALSE}
plot(resid(lmfit), type="l", col="blue")

lmfit2 <- lm(y~.+I(t^2), data=lmData) #+ I(exp(t-500))
anova(lmfit2)
summary(lmfit2)
plot(x)
points(seq(1960, 2022, length=length(x)),fitted(lmfit2), type="l", col="red")
res = resid(lmfit2)
plot(resid(lmfit2), type="l", col="red")
```


## Theory around differencing 

Assuming a linear trend with seasonal effect ($s=12$), we can decompose our data as $X_t = Y_t + T_t + S_t$, where:

- $X_t$ is the temperature

- $S_t = S_{t+12}$ is the seasonal component 

- $T_t = at +b$ is the trend component (and a, b are both fixed constants) 

- $Y_t$ is the stationary remainder with mean zero 

We can observe that (with $B$ the backshift operator)  : 

$$(1-B)(1-B^{12}) X_t = (1-B)(1-B^{12}) (Y_t + T_t + S_t) = (1-B) (Y_t - Y_{t-12} + T_t - T_{t -12})$$
$$= (1-B) (Y_t - Y_{t-12} + at +b - at - 12a - b) = Y_t - Y_{t-1} - Y_{t-12} + Y_{t-13}$$

$$= (1 - B) (1 - B^{12}) Y_t$$

It can indeed be observed that after differentiating with lag 12 and 1, we obtain something that appears to be more stationary as shown in the Figure 2. Here again there is no more apparent trend or seasonality and the series seems to have zero mean which is very encouraging. We will thus construct a model on this differentiated series. 

```{r, fig.align="center", fig.cap="Figure 2: Process obtained after lag-1 and lag-12 differencing."}
x_diff <- diff(x, lag=12)
x_diff <- diff(x_diff)
plot(x_diff)

#res <- resid(lmfit)
#plot(resid(lmfit), type="l")
```

## Construction of the model

As enumerate before, we aim to fit a SARIMA model, we just determine that d=1 and D=1. In order to estimate the others parameters we study the ACF and PACF of this resulting process : 

```{r, fig.align="center", fig.cap="Figure 3: ACF and PACF of the process."}
par(mfrow = c(1, 2))
acf(x_diff, lag.max=25)
pacf(x_diff, lag.max=25)

#acf(res, lag.max=25)
#pacf(res, lag.max=25)

#par(mfrow = c(1))
```

We now need to determine the p, q, P, and Q parameters for our model. Specifically, we can observe that there are 3 or 4 significant pics in the plot (depending if we include those that slightly exceed the blue line), with the spike at lag 12 being particularly prominent, and another at lag 24. This leads us to conclude that q=3 or 4, and Q=2 or 1.

Similarly, we can use the PACF plot to determine the p and P parameters. A spike at lag k in the PACF plot indicates a correlation between $X_t$ and $X_{t+k}$, after removing the influence of all intermediate observations $X_{t+1}$, $X_{t+2}$, ..., $X_{t+k-1}$. After examining the PACF plot, we observe three significant spikes at the beginning and two more at lags 12 and 24. This leads us to conclude that p=3 and P=2.

In conclusion we obtain two possible models, depending on the confidence level we impose :

`(p, d, q, P, D, Q, s) = (3, 1, 4, 2, 1, 2, 12)` or `(p, d, q, P, D, Q, s) = (3, 1, 3, 2, 1, 1, 12)`

We can then fit the two models with the `arima` function. The function will estimate the parameters by Gaussian MLE. 

```{r, eval=FALSE}
fit_obs <- arima(x, order=c(3, 1, 3), seasonal=list(order=c(2, 1, 2), period=12))
AIC(fit_obs)
#acf(fit_obs$residuals); pacf(fit_obs$residuals)
```

```{r}
fit_obs1 <- arima(x, order=c(3, 1, 4), seasonal=list(order=c(2, 1, 1), period=12))
#AIC(fit_obs1)
#acf(fit_obs$residuals); pacf(fit_obs$residuals)
```

```{r}
fit_obs <- arima(x, order=c(3, 1, 3), seasonal=list(order=c(2, 1, 1), period=12))
#AIC(fit_obs)
#acf(fit_obs$residuals); pacf(fit_obs$residuals)
```

When fitting a SARIMA model to a time series, one common way to compare different models is to look at their AIC values. A lower AIC indicates a better model fit, so we might expect the model with the smaller AIC to be the better choice. In this case, the AIC values are respectively : `r round(AIC(fit_obs1), 2)` and `r round(AIC(fit_obs), 2)`. And it therefore appears that the second one is the most appropriate for the data. 

## Residual Diagnostics

In this section we will reproduce the exact same diagnostics that we did in the report. 

So the first one is related to the fact that the residuals of the fitting model should be roughly Gaussian. And we therefore make a QQ-plot of the residuals, which is shown in the Figure below. And as we can see they form indeed a straight line which is a very encouraging result. 


```{r, fig.align="center", fig.cap="Figure 4: QQ-plot of the model's residuals."}
qqnorm(fit_obs$residuals)
```

---

Moreover, we have to check that the residuals should be white noise. Then we plot the ACF and PACF, which are shown in Figure below. Here again, results are quite satisfying since there is no spike outside the blue lines. That suggests that there the terms are independent. 
However we can notice an unique spike on the first one in the ACF plot at lag 0, but this is normal since it represents the autocorrelation and by definition $\gamma(0)/\gamma(0) = 1$. 

```{r, fig.align="center", fig.cap="Figure 5: ACF and PACF of the residuals."}
par(mfrow = c(1, 2))
acf(fit_obs$residuals, lag.max=25, main="")
pacf(fit_obs$residuals, lag.max=25, main="")
#par(mfrow = c(1))
```

---

Another method to assess the quality of a fitted model is to simulate data from it. In this case, it means generating data from the year 1960 to 2022 and comparing it to the original data set that was used for fitting the model. The results of the simulation are very encouraging. The pink curve follows the almost every fluctuations of the process and has the same overall behavior.

```{r, fig.align="center", fig.cap="Figure : Comparison between the data set and the simulation from the fitted model."}
plot(x)
lines(seq(1960, 2022.9999, 1/12), fitted(fit_obs), , col="pink")
```













