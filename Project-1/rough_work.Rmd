---
title: "rough_work"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r}
library(ggplot2)
```

## Project 1

In this Project, we will analyze the data "snow_particles" from a Ph.D. student at the Laboratory of Cryospheric Sciences at EPFL, which lists the snow-flake diameters. One remark to make is to observe that we don't have access to the exact data, only some rounding: we know the number of particles in a given interval.

The **goal** here is to simulate snow-flake diameters that are compatible with the data. 

## First Explanatory of the data

Before to begin we need to explore the data, and the easiest way to do it is to plot the data. This way we will have a better idea of the problem in purpose. 

As only rounding is available, the plot represents on the X-axis the diameters (so the different length intervals) and the Y-axis the proportion. 
$%Dire que on a enlever la derniÃ¨re valeur ?--> bring no information since it is 0$

```{r, clean data} 
setwd("~/Google Drive/EPFL/Applied Stat/StatApp-314577/Project-1")
data <- read.csv2("1_snow_particles.csv", sep=",")
data <- data.frame(startpoint <- as.numeric(data$startpoint),
                   endpoint <- as.numeric(data$endpoint),
                   value <- as.numeric(data$retained....),
                   number <- as.numeric(data$particles.detected))
```

```{r, explanotory}
x <- (data$endpoint-data$startpoint)/2 + data$startpoint
data_clean <- data.frame(start=data$startpoint)
ggplot()+
  geom_point(aes(x[1:51], data$value[1:51]))

```

We can see observe that the curve has the forms of two two bumps (a big one and little one). This could imply that the bi-log-normal distribution cans be a good idea.
To recall the distribution of the log-normal distribution is for all x real (with real $\mu$ and positive $\sigma^2$):
$$f(x) = \frac{1}{x\sqrt{2 \pi \sigma^2}}\exp{(- \frac{(\ln(x) - \mu)^2}{2 \sigma^2})}$$
So the distribution of a bi-log-normal distribution is (with $\tau$ in [0, 1]):
$$f_2(x) = (1 - \tau) \frac{1}{x\sqrt{2 \pi \sigma_1^2}}\exp{(- \frac{(\ln(x) - \mu_1)^2}{2 \sigma_1^2})} + \tau \frac{1}{x\sqrt{2 \pi \sigma_2^2}}\exp{(- \frac{(\ln(x) - \mu_2)^2}{2 \sigma_2^2})} $$
The purpose now that the law is identified is to estimate the vector of parameter : $\theta = (\tau, \mu_1, \sigma_1^2, \mu_2, \sigma_2^2)$. And one way to do it is to use the EM-Algorithm.  

## EM - Algorithm 

The EM-Algorithm is indeed a popular tool for modeling multimodality and very useful when the estimation of the parameters by maximizing the likelihood is difficult, like in this case.
It is an iterative method, which aims to determine the parameters starting from random values and adapt them until the likelihood is close enough to the optimal. 

The algorithm process in two different steps : the "Expectation" and the "Maximization".

The E - step aims to estimate the unknown parameters given the data (X) and the values of the parameters at the previous iteration ($\hat{\theta}^{l-1}$). In other words, we need to calculate : 
$$\mathbb{E}_{\hat{\theta}^{l-1}}[l(\theta) | X = x] = Q (\theta, \hat{\theta}^{l-1})$$
In this case we obtain :
$$\begin{align*}
l(\theta) &= N_1 log(1 - \tau) + N_2 log(\tau) + \sum_{n=1}^N (1 - Z^{(n)}) log(f(x^{(n)}, \mu_1, \sigma_1)) + \sum_{n=1}^N Z^{(n)} log(f(x^{(n)}, \mu_2, \sigma_2)) 
\end{align*}$$

such that we obtain : 

$$\begin{align*}
 Q (\theta, \hat{\theta}^{l-1}) &= log(1 - \tau) (N - \sum_{n=1}^N p^{l-1}_n) + log(\tau) \sum_{n=1}^N p^{l-1}_n + \sum_{n=1}^N (1 - p^{l-1}_n) log(f(x^{(n)}, \hat{\mu}_1, \hat{\sigma}_1)) + \sum_{n=1}^N p^{l-1}_n log(f(x^{(n)}, \hat{\mu}_2, \hat{\sigma}_2)) 
\end{align*}$$

with $p^{l-1}_n = \mathbb{E}_{\hat{\theta}^{l-1}}[Z^{(n)} | X^{(n)} = x^{(n)}] = \frac{f(x^{(n)}, \hat{\mu}_2, \hat{\sigma}_2) \hat{\tau}^{l-1}}{f_2(x^{(n)}, \hat{\mu}_1, \hat{\mu}_2,  \hat{\sigma}_1, \hat{\sigma}_2, \hat{\tau}^{l-1})}$

The M - step aims to maximize the likelihood. And by deriving the previous formula by respectively $\tau, \mu_1, \mu_2, \sigma_1, \sigma_2$, we can remark that the maximizer are : $$\hat{\tau} = \frac{\sum_{n=1}^N p_n^{l-1}}{N}$$
$$\hat{\mu}_1 = \frac{\sum_{n=1}^N ln({x^{(n)})}(1 - p_n^{l-1})}{\sum_{n=1}^N 1 - p_n^{l-1}}$$
$$\hat{\mu}_2 = \frac{\sum_{n=1}^N ln({x^{(n)})}p_n^{l-1}}{\sum_{n=1}^N  p_n^{l-1}}$$
$$\hat{\sigma}_1^2 = \frac{\sum_{n=1}^N (ln({x^{(n)}) - \mu_1)^2}(1 - p_n^{l-1})}{\sum_{n=1}^N 1 - p_n^{l-1}}$$

$$\hat{\sigma}_2^2 = \frac{\sum_{n=1}^N (ln({x^{(n)})} - \mu_2)^2 p_n^{l-1}}{\sum_{n=1}^N  p_n^{l-1}}$$

```{r, bi-log-normal law & log likelihood}

f <- function(x, mu1, mu2, sigma1, sigma2, tau){
  return((1 - tau) * exp(- (log(x) - mu1)^2 / (2 * sigma1)) / (x * sqrt(2 * pi * sigma1))
         + tau * exp(- (log(x) - mu2)^2 / (2 * sigma2)) / (x * sqrt(2 * pi * sigma2)))
}

  # Other possibility :
f2 <- function(x, mu1, mu2, sigma1, sigma2, tau){
  return((1 - tau) * dnorm(log(x), mu1, sqrt(sigma1))/x 
         + tau * dnorm(log(x), mu2, sqrt(sigma2))/x )
}

log_likelihood <- function(X, mu1, mu2, sigma1, sigma2, tau){
  return (sum(log(tau * dnorm(log(X), mu2, sigma2) + (1-tau) * dnorm(log(X), mu1, sigma1)) - log(X)))
}
```

```{r, em-algo, eval=FALSE}
em <- function(tol, mu1_l, mu2_l, sigma1_l, sigma2_l, tau_l){
  mu1_t <- c(mu1_l)
  mu2_t <- c(mu2_l)
  sigma1_t <- c(sigma1_l)
  sigma2_t <- c(sigma2_l)
  tau_t <- c(tau_l)

  logl_prev <- 0 
  logl <- log_likelihood(X, mu1_l, mu2_l, sigma1_l, sigma2_l, tau_l)
  i <- 0
  while (abs(logl_prev - logl) > tol & i<100) {
    i <- i+1
    logl_prev <- logl
    p <- tau_l * dnorm(log(X), mu2_l, sigma2_l) / (X * f(X, mu1_l, mu2_l, sigma1_l, sigma2_l, tau_l))
    tau_l <- mean(p)
    tau_t <- c(tau_t, tau_l)
   
    mu1_l <- sum((1-p)*log(X))/sum(1-p)
    mu2_l <- sum(p*log(X))/sum(p)
    mu1_t <- c(mu1_t, mu1_l) 
    mu2_t <- c(mu2_t, mu2_l) 
  
    sigma1_l <- sqrt(sum((1-p)*(log(X) - mu1_l)^2)/sum(1-p))
    sigma2_l <- sqrt(sum(p * (log(X) - mu2_l)^2)/sum(p))
    sigma1_t <- c(sigma1_t, sigma1_l)
    sigma2_t <- c(sigma2_t, sigma2_l)
  
    logl <- log_likelihood(X, mu1_l, mu2_l, sigma1_l, sigma2_l, tau_l)
  }
  
  if (i>99){
    print("the algorithm didn't converge")
  }
  
  if (tau_l < 0.5){
    return(list(mu2_t, mu1_t, sigma2_t, sigma1_t, 1-tau_t))
  }
  return(list(mu1_t, mu2_t, sigma1_t, sigma2_t, tau_t))
}
```



```{r, data treatment}


```




